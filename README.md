# basic-machine-learning

Sources: https://www.simplilearn.com/tutorials/machine-learning-tutorial/machine-learning-interview-questions

- When the weights become large, a small change in the input can cause a large change in the output. This makes the model very sensitive to minor variations or noise in the training data. As a result, the model starts to learn these small, often irrelevant details—this is overfitting, where the model performs well on training data but poorly on unseen data because it hasn't learned the underlying patterns, just the quirks of the training set. To combat this, regularization techniques like L1 and L2 add a penalty to the loss function that discourages large weights: L1 regularization adds a penalty equal to the sum of the absolute values of the weights; L2 regularization adds a penalty equal to the sum of the squares of the weights.

- A model with fewer or smaller weights is simpler, so it’s less capable of memorizing noise and more likely to generalize well to new, unseen data.

- K-Nearest Neighbors (KNN) is a simple, lazy learning algorithm used for classification and regression. It makes predictions by finding the K closest training examples to a new data point using a distance metric (like Euclidean distance). For classification, it predicts the majority class; for regression, it averages the values. Pros: simple, effective on small datasets. Cons: K is a key parameter (small K may overfit, large K may underfit); slow with large data, sensitive to irrelevant features, doesn't handle high dimensions well.

- Naive Bayes predicts the class of a data point based on the probability of each class given the input features. It applies Bayes’ Theorem: probability of class given the features = (probability of features given the class * prior probability of the class)/overall probability of the input features. It assumes that all features are independent given the class, which is rarely true in practice—hence the “naive” part. But the model often works surprisingly well anyway! Pros: Fast, simple, and works well for text classification. Cons: Can perform poorly when features are highly correlated.

- Association is about finding patterns between items in large datasets. It's commonly used in market basket analysis — understanding which items are frequently bought together.

- A recommendation system suggests relevant items to users by analyzing their preferences, behavior, or similarities with others. There are three main types: Collaborative Filtering – Uses patterns in user behavior to recommend items based on what similar users have liked; Content-Based Filtering – Recommends items similar to those the user has shown interest in, using item features like genre, category, or description; Hybrid Methods – Combine both collaborative and content-based techniques to improve accuracy and overcome individual limitations.

- In a Decision Tree Classifier, the data is recursively split into smaller subsets based on feature values as the tree is built. Each internal node represents a decision point — a test or split based on a specific feature and its value. The branches coming from a node represent the possible outcomes of that decision (e.g., "Yes"/"No" or different value ranges). Finally, the leaf nodes represent the predicted class labels — the outcome of following a path through the tree. Pros: Easy to understand and interpret; Handles both numerical and categorical data. Cons: Prone to overfitting if not pruned.

- An SVM is a powerful supervised learning algorithm used for classification (and regression). Its goal is to find the best boundary (hyperplane) that separates classes of data with the maximum margin. Standard (linear) SVMs work well only when the data is linearly separable — that is, when a straight line (or plane) can divide the classes. But real-world data is often not linearly separable. A Kernel SVM solves this problem by using a kernel function to transform the data into a higher-dimensional space where a linear separation is possible. Cons: Can be computationally expensive, especially with large datasets; Needs careful tuning of kernel and hyperparameters like C and γ.

- Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex datasets while retaining as much important information (variance) as possible.

- Feedforward neural networks treat all inputs as independent, processing data in one direction without memory. In contrast, RNNs are designed for sequential data, where the order of inputs matters. RNNs maintain a hidden state that acts as memory, enabling the model to retain information from previous time steps. At each step t, the hidden state h_t is computed based on the current input x_t and the previous hidden state h_(t−1)​, using shared weights across time steps. The output is typically derived from this hidden state. This architecture allows RNNs to capture temporal dependencies, making them suitable for tasks like audio analysis, language modeling, and time series prediction.

- XGBoost is a gradient boosting framework that builds an ensemble of decision trees in a stage-wise manner to optimize a specified loss function. The process begins with a simple initial prediction, typically the mean of the target variable. It then computes residuals—the difference between the actual and predicted values—and constructs a decision tree to model these residuals. In each subsequent iteration, XGBoost adds a new tree that focuses on correcting the remaining errors from the previous ensemble, gradually improving the model’s performance. Throughout training, XGBoost minimizes a differentiable loss function using both first- and second-order gradients (i.e., the gradient and Hessian), allowing for efficient and precise optimization. Regularization is applied to penalize overly complex trees, helping to prevent overfitting. The process continues until stopping criteria are met, such as reaching a maximum number of trees or achieving minimal improvement in loss. Pros: XGBoost offers high predictive accuracy by combining multiple decision trees through boosting, making it a powerful choice for both regression and classification tasks. It is highly scalable, efficient on large datasets, and supports parallel processing. Additionally, it provides some model interpretability through feature importance scores. Cons: XGBoost is more complex than simpler models and can be prone to overfitting if not carefully tuned. It also tends to use more memory due to its ensemble structure and may not perform optimally on extremely high-dimensional or sparse datasets compared to specialized algorithms.

- In ST-GCN, human motion is represented as a spatio-temporal graph, where nodes are body joints and edges represent anatomical connections (e.g., bones) and temporal links across frames. The model applies two main operations: 1. Spatial Graph Convolution: For each time step (i.e., each frame), a graph convolution is applied across the joints. A kernel aggregates information from a joint and its connected neighbors, extracting spatial features that capture body pose. This is analogous to a CNN scanning local patches in an image but adapted to a graph structure. 2. Temporal Convolution: After spatial features are extracted for each joint across all frames, a 1D convolution is applied along the time axis for each joint individually. This captures how the features of a joint evolve over time, allowing the model to detect dynamic motion patterns (e.g., freezing, walking). Together, these two operations allow ST-GCN to learn both the structural configuration of the body and its movement over time — making it well-suited for tasks like freezing of gait detection.

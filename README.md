# basic-machine-learning

Sources: https://www.simplilearn.com/tutorials/machine-learning-tutorial/machine-learning-interview-questions

- When the weights become large, a small change in the input can cause a large change in the output. This makes the model very sensitive to minor variations or noise in the training data. As a result, the model starts to learn these small, often irrelevant details—this is overfitting, where the model performs well on training data but poorly on unseen data because it hasn't learned the underlying patterns, just the quirks of the training set. To combat this, regularization techniques like L1 and L2 add a penalty to the loss function that discourages large weights: L1 regularization adds a penalty equal to the sum of the absolute values of the weights; L2 regularization adds a penalty equal to the sum of the squares of the weights.

- A model with fewer or smaller weights is simpler, so it’s less capable of memorizing noise and more likely to generalize well to new, unseen data.

- K-Nearest Neighbors (KNN) is a simple, lazy learning algorithm used for classification and regression. It makes predictions by finding the K closest training examples to a new data point using a distance metric (like Euclidean distance). For classification, it predicts the majority class; for regression, it averages the values. Pros: simple, effective on small datasets. Cons: K is a key parameter (small K may overfit, large K may underfit); slow with large data, sensitive to irrelevant features, doesn't handle high dimensions well.

- Naive Bayes predicts the class of a data point based on the probability of each class given the input features. It applies Bayes’ Theorem: probability of class given the features = (probability of features given the class * prior probability of the class)/overall probability of the input features. It assumes that all features are independent given the class, which is rarely true in practice—hence the “naive” part. But the model often works surprisingly well anyway! Pros: Fast, simple, and works well for text classification. Cons: Can perform poorly when features are highly correlated.

- Association is about finding patterns between items in large datasets. It's commonly used in market basket analysis — understanding which items are frequently bought together.

- A recommendation system suggests relevant items to users by analyzing their preferences, behavior, or similarities with others. There are three main types: Collaborative Filtering – Uses patterns in user behavior to recommend items based on what similar users have liked; Content-Based Filtering – Recommends items similar to those the user has shown interest in, using item features like genre, category, or description; Hybrid Methods – Combine both collaborative and content-based techniques to improve accuracy and overcome individual limitations.

- In a Decision Tree Classifier, the data is recursively split into smaller subsets based on feature values as the tree is built. Each internal node represents a decision point — a test or split based on a specific feature and its value. The branches coming from a node represent the possible outcomes of that decision (e.g., "Yes"/"No" or different value ranges). Finally, the leaf nodes represent the predicted class labels — the outcome of following a path through the tree. Pros: Easy to understand and interpret; Handles both numerical and categorical data. Cons: Prone to overfitting if not pruned.

- An SVM is a powerful supervised learning algorithm used for classification (and regression). Its goal is to find the best boundary (hyperplane) that separates classes of data with the maximum margin. Standard (linear) SVMs work well only when the data is linearly separable — that is, when a straight line (or plane) can divide the classes. But real-world data is often not linearly separable. A Kernel SVM solves this problem by using a kernel function to transform the data into a higher-dimensional space where a linear separation is possible. Cons: Can be computationally expensive, especially with large datasets; Needs careful tuning of kernel and hyperparameters like C and γ.

- Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex datasets while retaining as much important information (variance) as possible.
